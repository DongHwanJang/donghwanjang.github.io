<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Dong-Hwan Jang</title>
  <meta name="author" content="Dong-Hwan Jang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>
<body>
  <table class="page-container"><tbody>
    <tr>
      <td>
        <!-- Header: name + photo -->
        <table><tbody>
          <tr>
            <td class="cell-pad-sm cell-63">
              <p class="text-center">
                <name>Dong-Hwan Jang</name>
              </p>
              <p>
                I am <b>Dong-Hwan Jang</b>, a first-year Ph.D. student in Computer Science at the <a href="https://illinois.edu">University of Illinois Urbana-Champaign (UIUC)</a>. My research centers on bridging <b>3D/4D vision and generative modeling</b>, with a focus on building <b>robust, physics-grounded visual systems</b> that can maintain <b>spatial and temporal consistency</b>. I explore how <b>3D geometry and physical priors</b> can guide controllable video and image generation, and conversely, how <b>generative models</b> can enhance the adaptability of <b>3D/4D scene representations</b>. This work extends my earlier research on robust and efficient vision systems, including the design of adaptive network modules (such as the <i>DynOPool</i> pooling layer and an <i>implicit deblurring module</i>) and improving model robustness through <b>weight merging</b>.
              </p>
              <p>
                Before starting my Ph.D., I worked as an AI researcher at Samsung, where I researched OOD-robust fine-tuning techniques on domain-specific data. I earned my M.S. in Electrical and Computer Engineering from <a href="https://en.wikipedia.org/wiki/Seoul_National_University">Seoul National University</a>, where I was advised by Professor <a href="https://cv.snu.ac.kr/index.php/~bhhan/">Bohyung Han</a>, and also interned at <a href="https://naver-career.gitbook.io/en/teams/clova-cic/ai-lab">NAVER AI Lab</a> with <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a> and <a href="https://sangdooyun.github.io">Sangdoo Yun</a>.
              </p>
              <p class="text-center">
                <a href="mailto:djang12@illinois.edu">Email</a> &nbsp/&nbsp
                <a href="data/DongHwan_Jang_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=5MLvB1YAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/dhjang10">X</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/dong-hwan-jang-b38283181">Linkedin</a>
              </p>
            </td>
            <td class="cell-40">
              <a href="images/ny_jdh_photo.png"><img class="profile-photo hoverZoomLink" alt="profile photo" src="images/ny_jdh_photo_circular.png"></a>
            </td>
          </tr>
        </tbody></table>

        <!-- News -->
        <table class="section-block section-block-first" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>üöÄ News</heading>
              <ul class="news-list">
                <li class="news-recent"><span class="news-date">Feb 2026</span> <span class="news-badge">New!</span> Two papers accepted at CVPR 2026: <b>RewardFlow</b>, <b>PyraTok</b>.</li>
                <li><span class="news-date">Aug 2025</span> Started Ph.D. in Computer Science at <a href="https://illinois.edu">UIUC</a>.</li>
                <li><span class="news-date">2024‚Äì2025</span> Worked as AI researcher at Samsung.</li>
                <li><span class="news-date">2025</span> Patent filed on heterogeneous model merging technique (KR, CN, US, EU) at Samsung.</li>
                <li><span class="news-date">Oct 2024</span> <a href="https://eccv.ecva.net/virtual/2024/poster/2359">Model Stock</a> accepted at ECCV 2024 (<span style="color: #c41e3a;">Oral, top 2.3%</span>).</li>
                <li><span class="news-date">Jun 2024</span> <a href="https://arxiv.org/abs/2511.21490">Merge and Bound</a> accepted at CVPR 2024 CL Workshop.</li>                
              </ul>
            </td>
          </tr>
        </tbody></table>

        <!-- Education -->
        <table class="section-block" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
              <ul class="news-list">
                <li><strong>Ph.D. Student, Computer Science</strong>, University of Illinois Urbana-Champaign (UIUC) <span class="edu-date">(Aug 2025 ‚Äì Present)</span> ¬∑ Research in robust and efficient 3D/4D vision</li>
                <li><strong>Visiting Scholar</strong>, Carnegie Mellon University <span class="edu-date">(Sep 2022 - Feb 2023)</span> ¬∑ Full-time, Pittsburgh, PA ¬∑ AI project, fully funded by the Korean Government</li>
                <li><strong>Master's Student, ECE (Computer Vision)</strong>, Seoul National University <span class="edu-date">(Sep 2020 - Aug 2023)</span> ¬∑ Research advised by Prof. Bohyung Han</li>
                <li><strong>Bachelor's Degree, ECE / MOT</strong>, Seoul National University <span class="edu-date">(Mar 2013 - Aug 2020)</span> ¬∑ <strong>Summa Cum Laude</strong>, ranked <strong>1st in class</strong></li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <!-- Research -->
        <table><tbody>
          <tr>
            <td class="cell-75" style="padding:20px;">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>

        <table><tbody>
          <tr class="paper-row row-highlight">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/RewardFlow_after.png" width="180" alt=""></div>
                <img src="images/RewardFlow_before.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <papertitle>RewardFlow: Generate Images by Optimizing What You Reward</papertitle>
              <br>
              Onkar Kishor Susladkar, <strong>Dong-Hwan Jang</strong>, Tushar Prakash, Adheesh Sunil Juvekar, Vedant Shah, Ayush Barik, Nabeel Bashir, Muntasir Wahed, Ritish Shrirao, <a href="https://isminoula.github.io">Ismini Lourentzou</a>
              <br>
              <em>CVPR</em>, 2026
              <p>RewardFlow is a zero-shot, training-free framework for text-guided image editing and generation using reward-guided Langevin dynamics. We steer pretrained diffusion and flow-matching models at inference with hierarchically designed coarse-to-fine differentiable rewards (e.g., a VQA-based reward for semantic supervision and a SAM-guided reward for localized edits), controlled by a prompt-aware adaptive policy. </p>
            </td>
          </tr>

          <tr class="paper-row">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/pyratok_after.png" width="180" alt=""></div>
                <img src="images/pyratok_before.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <papertitle>PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation</papertitle>
              <br>
              Onkar Kishor Susladkar, Tushar Prakash, Adheesh Sunil Juvekar, Kiet A. Nguyen, <strong>Dong-Hwan Jang</strong>, Inderjit S. Dhillon, <a href="https://isminoula.github.io">Ismini Lourentzou</a>
              <br>
              <em>CVPR</em>, 2026
              <p>PyraTok is a language-aligned pyramidal tokenizer for video understanding and generation that learns discrete latents across multiple spatial-temporal resolutions. We introduce Language-aligned Pyramidal Quantization (LaPQ), discretizing encoder features at several depths with a shared large binary codebook. PyraTok achieves state-of-the-art video reconstruction, text-to-video quality, and zero-shot performance on segmentation, action localization, and understanding, scaling to 4K/8K resolutions.</p>
            </td>
          </tr>

          <tr class="paper-row row-highlight">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/modelstock_after.png" width="180" alt=""></div>
                <img src="images/modelstock_before.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <a href="https://eccv.ecva.net/virtual/2024/poster/2359"><papertitle>Model Stock: All we need is just a few fine-tuned models</papertitle></a>,
              <br>
              <strong>Dong-Hwan Jang</strong>, <a href="https://sangdooyun.github.io/">Sangdoo Yun</a>, <a href="https://sites.google.com/site/dyhan0920/">Dongyoon Han</a>
              <br>
              <em>ECCV, 2024 (<span style="color: #c41e3a;">Oral, Top 2.3% among submitted papers</span>)</em>.
              <br><br>
              <a href="https://arxiv.org/abs/2403.19522">arxiv</a> / <a href="https://github.com/naver-ai/model-stock">github</a> / <a href="https://www.marktechpost.com/2024/04/01/naver-ai-lab-introduces-model-stock-a-groundbreaking-fine-tuning-method-for-machine-learning-model-efficiency/">article (marktechpost)</a>
              <p>Thanks to our insights in the fine-tuned weight space, fine-tuning a few models (i.e., only two) can lead to superior merged weights (closer to the center of a weight space) without merging many fine-tuned models under extensive parameter searches like Model Soup.</p>
            </td>
          </tr>

          <tr class="paper-row">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/mb_after.png" width="180" alt=""></div>
                <img src="images/mb_before.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <a href="https://arxiv.org/abs/2511.21490"><papertitle>Merge and Bound: Direct Manipulations on Weights for Class Incremental Learning</papertitle></a>
              <br>
              <a href="https://sites.google.com/view/taehoonkim0723">Taehoon Kim</a>, <strong>Dong-Hwan Jang</strong>, <a href="https://cv.snu.ac.kr/index.php/~bhhan/">Bohyung Han</a>
              <br>
              <em>CVPR Workshop on <a href="https://sites.google.com/view/clvision2024">Continual Learning in Computer Vision</a></em>, 2024
              <p>We introduce Merge-and-Bound (M&B), an innovative approach for Class Incremental Learning that optimizes model weights through two merging techniques: inter-task and intra-task weight merging, alongside a bounded update to prevent catastrophic forgetting. Without altering architectures or objectives, M&B integrates into existing methods, showing superior performance on CIL benchmarks against top competitors.</p>
            </td>
          </tr>

          <tr class="paper-row row-highlight">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/dynopool_after.gif" width="180" alt=""></div>
                <img src="images/dynopool_before.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <a href="https://arxiv.org/pdf/2205.15254.pdf"><papertitle>Pooling Revisited: Your Receptive Field is Suboptimal</papertitle></a>
              <br>
              <strong>Dong-Hwan Jang</strong>, Sanghyeok Chu, Joonhyuk Kim, <a href="https://cv.snu.ac.kr/index.php/~bhhan/">Bohyung Han</a>
              <br>
              <em>CVPR</em>, 2022
              <p>We propose <em>DynOPool</em>, a learnable pooling layer that finds the optimal scale factors and receptive fields of intermediate feature maps. It adapts feature map sizes and shapes for enhanced accuracy and efficiency in tasks like image classification and semantic segmentation.</p>
            </td>
          </tr>

          <tr class="paper-row">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/dataset-cover.png" width="180" alt=""></div>
                <img src="images/dataset-cover.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <papertitle>DS4C Patient Policy Province Dataset: a Comprehensive COVID-19 Dataset for Causal and Epidemiological Analysis</papertitle>
              <br>
              <a href="https://www.linkedin.com/in/jimi-kim/">Jimi Kim*</a>, Seojin Jang*, <a href="https://www.linkedin.com/in/joong-kun-lee-0a2477b6/">Joong Kun Lee*</a>, <strong>Dong-Hwan Jang*</strong> (* equal contributions)
              <br>
              <em>NeurIPS Workshop on <a href="https://www.cmu.edu/dietrich/causality/neurips20ws/">Causal Discovery & Causality-Inspired Machine Learning</a></em>, 2020
              <br><br>
              <a href="https://www.kaggle.com/kimjihoo/coronavirusdataset">project page</a> / <a href="http://www.aitimes.com/news/articleView.html?idxno=136249">article (korean)</a> <a href="https://papago.naver.net/website?locale=en&source=ko&target=en&url=http%3A%2F%2Fwww.aitimes.com%2Fnews%2FarticleView.html%3Fidxno%3D136249">(auto-translated)</a>
              <p>We present DS4C South Korea Patient, Policy, and Provincial data (DS4C-PPP dataset). The dataset contains comprehensive data that could be used for causal analysis, such as per-patient symptom onset and confirmed date, travel frequency, hospital accessibility, and 61 preventative policies enacted in South Korea.</p>
            </td>
          </tr>
        </tbody></table>

        <!-- Academic Projects -->
        <table class="section-block" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr><td><heading>Academic Projects</heading></td></tr>
        </tbody></table>

        <table class="academic-projects-table"><tbody>
          <tr class="paper-row">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/diff_after.png" width="180" alt=""></div>
                <img src="images/diff_before.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <papertitle>DIFF: Deblurring Implicit Feature Function</papertitle>
              <p>We propose spatially-variant motion deblur network based on the implicit neural representation. A spatially-variant deblurring network takes deformed features and their offsets as inputs.</p>
              <i>U.S. Patent Application Number: 17/973,809 (in progress)</i>
            </td>
          </tr>

          <tr class="paper-row">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/hier.gif" width="180" alt=""></div>
                <img src="images/hier_3.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <papertitle>Dynamic Spatially-Adaptive Modulation for image Dehazing</papertitle>
              <p>We propose a novel framework of the dynamic spatially-adaptive modulation for image dehazing. The proposed algorithm introduces a selection module that conditionally determine the necessary modulation pathways in a bottom-up manner by providing a loss function optimizing both accuracy and efficiency.</p>
            </td>
          </tr>

          <tr class="paper-row">
            <td class="cell-25">
              <div class="one">
                <div class="two"><img src="images/depthfinder.png" width="180" alt=""></div>
                <img src="images/depthfinder.png" width="180" alt="">
              </div>
            </td>
            <td class="cell-75">
              <papertitle>DepthFinder: Universal Image Restoration based on Adaptive Inference</papertitle>
              <p>We adaptively find the appropriate number of the residual blocks according to the severity and distortion type of the input in universal image restoration task.</p>
            </td>
          </tr>
        </tbody></table>

        <hr>

        <!-- Scholarships & Award -->
        <table class="section-block" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Scholarships & Award</heading>
              <ul class="scholarship-list">
                <li>
                  <strong>Kwanjeong Educational Foundation Study Abroad Scholarship</strong> (2025‚Äì2029)
                  <span class="scholarship-desc">Provides USD 25,000 per year to support Ph.D. study, awarded to outstanding Korean students pursuing advanced research overseas.</span>
                </li>
                <li>
                  <strong>Korean Government Scholarship for Overseas Study</strong> (2023‚Äì2024)
                  <span class="scholarship-desc">Covers USD 40,000 support per year. Only 64 students are selected in all fields in Korea.</span>
                </li>
                <li>
                  <strong>Hyundai OnDream Global Scholarship Award</strong> (2022)
                  <span class="scholarship-desc">Award Prize ‚Äì around USD 2,350 for the paper "Pooling Revisited: Your Receptive Field is Suboptimal" at CVPR 2022.</span>
                </li>
                <li>
                  <strong>Hyundai OnDream Future Technology Scholarship</strong> (2021‚Äì2022)
                  <span class="scholarship-desc">Covers full tuition & financial support.</span>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <!-- Talks -->
        <table class="section-block" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Talks</heading>
              <ul class="scholarship-list">
                <li>
                  <a href="https://kcvs.kr/?act=info.workshop&pseq=3">Korean Conference on Computer Vision 2022</a>
                  <span class="scholarship-desc">20 minutes oral presentation (top 23.5% among published papers) on CVPR paper "Pooling Revisited: Your Receptive Field is Suboptimal" presented by prof. Bohyung Han</span>
                </li>
                <li>
                  <a href="https://www.youtube.com/watch?v=pVrbEGMo3_g">Databricks Invited Talk</a>
                  <span class="scholarship-desc">1 hour talk on "The Complexities around COVID-19 Data" invited as DS4C team</span>
                </li>
              </ul>
            </td>
          </tr>
        </tbody></table>

        <!-- Teaching (commented out - uncomment when needed)
        <table class="section-block" width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching Experiences</heading>
              <ul class="news-list">
                <li>Teaching Assistant for 430.329: Introduction to Algorithms at Seoul National University (Fall 2020)</li>
                <li>Teaching Assistant for Samsung AI Expert Course at Seoul National University (July 2019)</li>
                <li>Teaching Assistant for Hyundai Motors AI Expert Course at Seoul National University (Jan 2019)</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        -->

        <footer>
          <p style="text-align:right;font-size:small;">
            Template: <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>, Last update: 2/2026.
          </p>
        </footer>
      </td>
    </tr>
  </table>
</body>
</html>
